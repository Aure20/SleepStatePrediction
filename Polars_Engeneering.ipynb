{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef7e330d-72d3-48d5-ada5-9ad60d0a0af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import polars as pl\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "\n",
    "column_names = {\n",
    "    'series_id_column_name': 'series_id',\n",
    "    'time_column_name': 'step',\n",
    "    'event_column_name': 'event',\n",
    "    'score_column_name': 'score',\n",
    "}\n",
    "\n",
    "tolerances = {\n",
    "    'onset': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360], \n",
    "    'wakeup': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f630a6ec-650e-4001-88d2-2137c9c996fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For a quantile and a dataframe, returns a list containign the distances to the previous/next element above that quantile\n",
    "def return_dist(df, p, len_df):\n",
    "    steps_high = df.filter(pl.col('enmo') > pl.quantile('enmo',p)).select('step')\n",
    "    steps_high = steps_high.to_numpy().flatten()\n",
    "    steps_high = np.append((np.insert(steps_high,0,0)),len_df)\n",
    "    to_next = np.diff(steps_high).tolist()\n",
    "    steps_to_next = []\n",
    "    steps_to_prev = []\n",
    "    for i, val in enumerate(to_next[1:]):\n",
    "        val = int(val)\n",
    "        to_extend = range(0, val)\n",
    "        if i == 0:\n",
    "            half_val = int(val / 2)\n",
    "            steps_to_next.extend(list(reversed(to_extend)))\n",
    "            steps_to_prev.extend(list(range(0+half_val, val+half_val)))\n",
    "            continue\n",
    "        if i == len(to_next)-2:\n",
    "            half_val = int(val / 2)\n",
    "            steps_to_next.extend(list(reversed(range(0+half_val, val+half_val))))\n",
    "            steps_to_prev.extend(list(to_extend))\n",
    "            break\n",
    "        steps_to_next.extend(list(reversed(to_extend)))\n",
    "        steps_to_prev.extend(list(to_extend))\n",
    "    \n",
    "    last_next = 100\n",
    "    last_prev = 100\n",
    "    for item in reversed(steps_to_next):\n",
    "        if item is not None:\n",
    "            last_next = item\n",
    "            break\n",
    "    for item in reversed(steps_to_prev):\n",
    "        if item is not None:\n",
    "            last_prev = item    \n",
    "            break\n",
    "            \n",
    "    steps_to_next.extend(reversed(range(last_next,last_next+(len_df-len(steps_to_next)))))\n",
    "    steps_to_prev.extend(range(last_prev,last_prev+(len_df-len(steps_to_prev))))\n",
    "    \n",
    "    return pl.Series(values = steps_to_next), pl.Series(values = steps_to_prev), last_next, last_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2321b16-1db4-45e5-a988-5fc2ef2eb91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_awake = dict(zip(range(1440), np.sin(np.linspace(0, np.pi, 1440) + 0.208 * np.pi) ** 24))\n",
    "signal_onset = dict(zip(range(1440), np.sin(np.linspace(0, np.pi, 1440) + 0.555 * np.pi) ** 24))\n",
    "\n",
    "def custom_feat_eng(df:pl.DataFrame, to_keep:list[str])->pl.DataFrame:\n",
    "    len_df = len(df) #It sucks to have to do this only for the height \n",
    "    #Column arrays\n",
    "    Prefab = []\n",
    "    Entropy = []\n",
    "    Distance = []\n",
    "    Movement = []\n",
    "    \n",
    "    \n",
    "    lids = np.maximum(pl.col('enmo')-0.02, 0.).rolling_sum(window_size = 120, center=True, min_periods=1)\n",
    "    df = df.with_columns((100/(lids+1)).rolling_mean(window_size = 360, center=True, min_periods=1).cast(pl.Float32).alias('lids'))\n",
    "    \n",
    "    \n",
    "    # Time related features\n",
    "    Time_columns = [\n",
    "    #pl.col('timestamp').str.to_datetime(),  \n",
    "    pl.col('timestamp').dt.hour().cast(pl.UInt8).alias('hour'),\n",
    "    pl.col('timestamp').dt.minute().cast(pl.UInt8).alias('minute')\n",
    "    ]\n",
    "    \n",
    "    #Not included 'lids_diffabs','lids_diff',enmo_diffabs'\n",
    "    Base_signals = [\n",
    "        (pl.col('hour') * 60 + pl.col('minute')).cast(pl.UInt16).map_dict(signal_awake).cast(pl.Float32).alias('signal_awake'),\n",
    "        (pl.col('hour') * 60 + pl.col('minute')).cast(pl.UInt16).map_dict(signal_onset).cast(pl.Float32).alias('signal_onset'),\n",
    "        pl.col('anglez').abs().cast(pl.Float32).alias('anglezabs'),\n",
    "        pl.col('anglez').diff().cast(pl.Float32).alias('anglez_diff'), \n",
    "        pl.col('enmo').diff().cast(pl.Float32).alias('enmo_diff'), \n",
    "        pl.col('anglez').diff().abs().cast(pl.Float32).alias('anglez_diffabs'), \n",
    "        (pl.col('enmo')*pl.col('lids')).cast(pl.Float32).alias('enmo_x_lids'),\n",
    "        (pl.col('anglez')*pl.col('enmo')).cast(pl.Float32).alias('anglez_x_enmo'),\n",
    "        (pl.col('lids')*pl.col('anglez')).cast(pl.Float32).alias('anglez_x_lids'),\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    rol_args = {'window_size':51, 'min_periods':1, 'center':True}\n",
    "    rol_args_2 = {'window_size':60, 'min_periods':1, 'center':True}\n",
    "    \n",
    "    for column in ['anglezabs','anglez_diff','enmo_diff','anglez_diffabs','enmo_x_lids',\n",
    "                   'anglez_x_lids','anglez_x_enmo','enmo', 'anglez', 'lids']:\n",
    "        n = 51\n",
    "        #Various rolling statistics (Modified kaggle code)\n",
    "        Prefab.extend([pl.col(column).diff(n).cast(pl.Float32).alias(f'{column}_diff_{n}'),\n",
    "                    pl.col(column).rolling_max(**rol_args).cast(pl.Float32).alias(f'{column}_diff_{n}_max'),\n",
    "                    pl.col(column).rolling_min(**rol_args).cast(pl.Float32).alias(f'{column}_min_{n}'),\n",
    "                    pl.col(column).rolling_max(**rol_args).cast(pl.Float32).alias(f'{column}_max_{n}'),\n",
    "                    pl.col(column).rolling_std(**rol_args).cast(pl.Float32).alias(f'{column}_std_{n}'),\n",
    "                    pl.col(column).rolling_mean(**rol_args).cast(pl.Float32).alias(f'{column}_mean_{n}'),\n",
    "                    pl.col(column).rolling_median(**rol_args).cast(pl.Float32).alias(f'{column}_median_{n}'),\n",
    "                    (pl.col(column).rolling_median(**rol_args)-pl.col(column)).abs().rolling_median(**rol_args).cast(pl.Float32).alias(f'{column}_mad_{n}'),\n",
    "                    (pl.col(column).rolling_min(**rol_args)-pl.col(column).rolling_max(**rol_args)).cast(pl.Float32).alias(f'{column}_amplit_{n}'),\n",
    "                    (pl.col(column).rolling_median(**rol_args_2)-pl.col(column)).abs().rolling_median(**rol_args_2).cast(pl.Float32).alias(f'{column}_mad_60')])\n",
    "    \n",
    "    #Other columns to add manually \n",
    "    Prefab.extend([pl.col('anglez').rolling_mean(window_size = 120, center=True, min_periods=1).diff(-120+1).cast(pl.Float32).alias('anglez_prev_600_diff'),\n",
    "            pl.col('anglez').rolling_std(**rol_args_2).cast(pl.Float32).alias(f'anglez_std_60'),\n",
    "            pl.col('enmo').rolling_mean(window_size = 120, center=True, min_periods=1).diff(-120+1).cast(pl.Float32).alias('enmo_prev_600_diff'),\n",
    "            pl.col('enmo').rolling_mean(**rol_args_2).cast(pl.Float32).alias(f'enmo_mean_60'),\n",
    "            pl.col('enmo').rolling_min(**rol_args_2).cast(pl.Float32).alias(f'enmo_min_60')])\n",
    "\n",
    "    \n",
    "    for col in ['enmo', 'anglez', 'lids']:\n",
    "        col_max = df.select(col).max() \n",
    "        col_min = df.select(col).min() \n",
    "        for b in [20,200]:\n",
    "            if df.get_column(col).n_unique() < b: #Corner case if there are not enough values\n",
    "                hist, bin_edges = np.histogram(df.get_column(col), bins=df.get_column(col).n_unique())\n",
    "                probabilities = hist / np.sum(hist)\n",
    "                digitized = np.digitize(df.get_column(col), bin_edges[:-1])-1\n",
    "                entropy_values = np.array([-probabilities[x]*np.log(probabilities[x]) for x in digitized]).astype(np.float32)\n",
    "                Entropy.append(pl.Series(values = entropy_values).alias(f'entropy_{b}_{col}'))  \n",
    "                continue\n",
    "            digitized = pl.col(col).cut(np.linspace(col_min,col_max,b,False)[1:], labels = [str(x) for x in range(b)]).cast(pl.Utf8).str.parse_int(10).cast(pl.UInt32).alias('digitized') #Need to cast cat dtype first to str and then int\n",
    "            digitized = df.select(digitized).to_series(0) \n",
    "            counted = digitized.value_counts(parallel = False)\n",
    "            probabilities = (digitized.map_dict(dict(zip(counted.to_series(0),counted.to_series(1)))))/len_df \n",
    "            probabilities = -probabilities*probabilities.log() \n",
    "            Entropy.append(probabilities.alias(f'entropy_{b}_{col}'))\n",
    "\n",
    "    \n",
    "    for p in [0.6, 0.9, 0.99]:\n",
    "        steps_to_next, steps_to_prev, last_next, last_prev = return_dist(df,p,len_df) \n",
    "        Distance.extend([steps_to_next.fill_nan(last_next).cast(pl.UInt32).alias(f'steps_to_next_{p}'), #Make sure that for loops don't break lazy eval\n",
    "                         steps_to_prev.fill_nan(last_prev).cast(pl.UInt32).alias(f'steps_to_prev_{p}'),\n",
    "                         steps_to_next.log1p().cast(pl.Float32).alias(f'log_steps_to_next_{p}'),\n",
    "                         steps_to_prev.log1p().cast(pl.Float32).alias(f'log_steps_to_prev_{p}')])\n",
    "        \n",
    "        for i in [24,120,400]:   \n",
    "                rol_args = {'window_size':i, 'min_periods':1, 'center':True}\n",
    "                mask = pl.when(pl.col('enmo') > pl.quantile('enmo',p)).then(1).otherwise(0)\n",
    "                Movement.append(mask.rolling_sum(**rol_args).cast(pl.Float32).alias(f'amount_of_movement_{p}_in_last_{i*5}'))\n",
    "    \n",
    "    df = df.lazy() #Once did all the computations on the dataframe optimize query      \n",
    "    df = df.with_columns(Time_columns)\n",
    "    df = df.with_columns(Base_signals)      \n",
    "    df = df.with_columns(Prefab)  \n",
    "    df = df.with_columns(Distance)  \n",
    "    df = df.with_columns(Movement) \n",
    "    df = df.with_columns(Entropy)\n",
    "    \n",
    "    df = df.select(pl.all().forward_fill()) \n",
    "    df = df.select(pl.all().backward_fill()) \n",
    "    df = df.select(pl.all().drop_nans()) \n",
    "\n",
    "    return df.select(to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "865a510f-9ae3-4bf3-9779-06a1be6aa270",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = ['series_id', 'step', 'timestamp']\n",
    "target_col  =['state']\n",
    "feature_cols = np.array(['anglez', 'enmo','lids', 'hour', 'minute', 'signal_awake', 'signal_onset', \n",
    "                'anglezabs', 'anglez_diff', 'enmo_diff', 'anglez_diffabs', 'enmo_x_lids', \n",
    "                'anglez_x_enmo', 'anglez_x_lids', 'anglezabs_diff_51', 'anglezabs_diff_51_max', \n",
    "                'anglezabs_min_51', 'anglezabs_max_51', 'anglezabs_std_51', 'anglezabs_mean_51',\n",
    "                'anglezabs_median_51', 'anglezabs_mad_51', 'anglezabs_amplit_51', \n",
    "                'anglezabs_mad_60', 'anglez_diff_diff_51', 'anglez_diff_diff_51_max', \n",
    "                'anglez_diff_min_51', 'anglez_diff_max_51', 'anglez_diff_std_51', \n",
    "                'anglez_diff_mean_51', 'anglez_diff_median_51', 'anglez_diff_mad_51', \n",
    "                'anglez_diff_amplit_51', 'anglez_diff_mad_60', 'enmo_diff_diff_51', \n",
    "                'enmo_diff_diff_51_max', 'enmo_diff_min_51', 'enmo_diff_max_51', \n",
    "                'enmo_diff_std_51', 'enmo_diff_mean_51', 'enmo_diff_median_51', \n",
    "                'enmo_diff_mad_51', 'enmo_diff_amplit_51', 'enmo_diff_mad_60', \n",
    "                'anglez_diffabs_diff_51', 'anglez_diffabs_diff_51_max', 'anglez_diffabs_min_51', \n",
    "                'anglez_diffabs_max_51', 'anglez_diffabs_std_51', 'anglez_diffabs_mean_51', \n",
    "                'anglez_diffabs_median_51', 'anglez_diffabs_mad_51', 'anglez_diffabs_amplit_51', \n",
    "                'anglez_diffabs_mad_60', 'enmo_x_lids_diff_51', 'enmo_x_lids_diff_51_max', \n",
    "                'enmo_x_lids_min_51', 'enmo_x_lids_max_51', 'enmo_x_lids_std_51', \n",
    "                'enmo_x_lids_mean_51', 'enmo_x_lids_median_51', 'enmo_x_lids_mad_51', \n",
    "                'enmo_x_lids_amplit_51', 'enmo_x_lids_mad_60', 'anglez_x_lids_diff_51', \n",
    "                'anglez_x_lids_diff_51_max', 'anglez_x_lids_min_51', 'anglez_x_lids_max_51', \n",
    "                'anglez_x_lids_std_51', 'anglez_x_lids_mean_51', 'anglez_x_lids_median_51',\n",
    "                'anglez_x_lids_mad_51', 'anglez_x_lids_amplit_51', 'anglez_x_lids_mad_60', \n",
    "                'anglez_x_enmo_diff_51', 'anglez_x_enmo_diff_51_max', 'anglez_x_enmo_min_51', \n",
    "                'anglez_x_enmo_max_51', 'anglez_x_enmo_std_51', 'anglez_x_enmo_mean_51', \n",
    "                'anglez_x_enmo_median_51', 'anglez_x_enmo_mad_51', 'anglez_x_enmo_amplit_51', \n",
    "                'anglez_x_enmo_mad_60', 'enmo_diff_51', 'enmo_diff_51_max', 'enmo_min_51', \n",
    "                'enmo_max_51', 'enmo_std_51', 'enmo_mean_51', 'enmo_median_51', 'enmo_mad_51', \n",
    "                'enmo_amplit_51', 'enmo_mad_60', 'anglez_diff_51', 'anglez_diff_51_max', \n",
    "                'anglez_min_51', 'anglez_max_51', 'anglez_std_51', 'anglez_mean_51', \n",
    "                'anglez_median_51', 'anglez_mad_51', 'anglez_amplit_51', 'anglez_mad_60', \n",
    "                'lids_diff_51', 'lids_diff_51_max', 'lids_min_51', 'lids_max_51', 'lids_std_51', \n",
    "                'lids_mean_51', 'lids_median_51', 'lids_mad_51', 'lids_amplit_51', 'lids_mad_60', \n",
    "                'anglez_prev_600_diff', 'anglez_std_60', 'enmo_prev_600_diff', 'enmo_mean_60', \n",
    "                'enmo_min_60', 'steps_to_next_0.6', 'steps_to_prev_0.6', 'log_steps_to_next_0.6', \n",
    "                'log_steps_to_prev_0.6', 'steps_to_next_0.9', 'steps_to_prev_0.9', \n",
    "                'log_steps_to_next_0.9', 'log_steps_to_prev_0.9', 'steps_to_next_0.99', \n",
    "                'steps_to_prev_0.99', 'log_steps_to_next_0.99', 'log_steps_to_prev_0.99', \n",
    "                'amount_of_movement_0.6_in_last_120', 'amount_of_movement_0.6_in_last_600', \n",
    "                'amount_of_movement_0.6_in_last_2000', 'amount_of_movement_0.9_in_last_120',\n",
    "                'amount_of_movement_0.9_in_last_600', 'amount_of_movement_0.9_in_last_2000', \n",
    "                'amount_of_movement_0.99_in_last_120', 'amount_of_movement_0.99_in_last_600', \n",
    "                'amount_of_movement_0.99_in_last_2000', 'entropy_20_enmo', 'entropy_200_enmo', \n",
    "                'entropy_20_anglez', 'entropy_200_anglez', 'entropy_20_lids', 'entropy_200_lids'])\n",
    "#Split array according to the original value where it's from\n",
    "enmo_feat = []\n",
    "anglez_feat = []\n",
    "lids_feat = []\n",
    "for feat in feature_cols:\n",
    "    if 'enmo' in feat:\n",
    "        enmo_feat.append(feat)\n",
    "    elif 'anglez' in feat:\n",
    "        anglez_feat.append(feat)\n",
    "    else:\n",
    "        lids_feat.append(feat)\n",
    "splits = {'enmo1': enmo_feat[:len(enmo_feat)//2]+target_col,\n",
    "         'enmo2': enmo_feat[len(enmo_feat)//2:]+target_col, \n",
    "         'anglez1': anglez_feat[:len(anglez_feat)//2]+target_col, \n",
    "         'anglez2': anglez_feat[len(anglez_feat)//2:]+target_col,\n",
    "         'lids1': lids_feat[:len(lids_feat)//2]+target_col,\n",
    "         'lids2': lids_feat[len(lids_feat)//2:]+target_col}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4404ebc8-8373-4a06-aa42-976d921173ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep = ['entropy_200_enmo',\n",
    " 'steps_to_next_0.99',\n",
    " 'steps_to_next_0.9',\n",
    " 'entropy_20_lids',\n",
    " 'entropy_20_anglez',\n",
    " 'hour',\n",
    " 'log_steps_to_next_0.99',\n",
    " 'log_steps_to_next_0.9',\n",
    " 'lids',\n",
    " 'signal_onset',\n",
    " 'signal_awake',\n",
    " 'log_steps_to_prev_0.99',\n",
    " 'entropy_200_lids',\n",
    " 'entropy_200_anglez',\n",
    " 'amount_of_movement_0.9_in_last_2000',\n",
    " 'anglez_diff_diff_51_max',\n",
    " 'anglezabs_max_51',\n",
    " 'enmo_x_lids_min_51',\n",
    " 'anglez_diff_mad_51',\n",
    " 'anglez_x_enmo_diff_51_max',\n",
    " 'amount_of_movement_0.99_in_last_2000',\n",
    " 'anglez_x_enmo_median_51',\n",
    " 'anglezabs',\n",
    " 'anglez',\n",
    " 'anglezabs_min_51',\n",
    " 'lids_mad_60']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e901c913-cbdd-429e-916b-99053096078d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[174, 275, 250, 41, 22]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "events = pd.read_csv('events_compressed.csv') \n",
    "counted_series = events['series_id'].value_counts().reset_index()\n",
    "counted_series.columns = ['series_id', 'count']\n",
    "most = list(counted_series['series_id'])[0:5]\n",
    "print(most)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b4f08ed-9e34-4ed1-b431-804f4e01a2d9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current split: app\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:17<00:00,  3.60s/it]\n"
     ]
    }
   ],
   "source": [
    "splits = {'app': to_keep+id_cols+target_col}\n",
    "\n",
    "for split,cols in splits.items():\n",
    "    print(f'Current split: {split}')\n",
    "    train = pl.scan_parquet(source = 'train_label.parquet',) #length of 127.946.340\n",
    "    chuncks = []\n",
    "    for idt in tqdm(most):\n",
    "        id_df = train.filter(pl.col('series_id') == idt).collect()\n",
    "        chuncks.append(custom_feat_eng(id_df,cols))\n",
    "    train = pl.concat(chuncks, how = 'vertical').collect()\n",
    "    train.write_parquet(f\"train_{split}.parquet\", compression = 'zstd', compression_level = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398f37b2-80ed-40f5-b3f9-ab65960ef8cb",
   "metadata": {},
   "source": [
    "Create timeframe for training fine segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a3aba36-2c4d-46a2-9323-e7220ea7a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_timeframe(df:pl.DataFrame, timeframe:int, event:pl.DataFrame, series_id: int, len_df: int)->pl.DataFrame:\n",
    "    event_steps = event.filter(pl.col('series_id') == series_id).select('step').to_numpy()\n",
    "    true_vals = []\n",
    "    target = []\n",
    "    for event_step in event_steps:\n",
    "        offset = random.randint(-timeframe//3,timeframe//3) #Small offset so that the split is not in the dead center, can be corrected \n",
    "    #by looking how far the tree prediction on average is\n",
    "        start_idx = event_step[0]-timeframe//2+offset\n",
    "        end_idx = event_step[0]+timeframe//2+offset\n",
    "        if start_idx >= 0 and end_idx<len_df and (len(true_vals) == 0 or start_idx>true_vals[-1]): #Avoid overlapping\n",
    "            true_vals += range(start_idx, end_idx)\n",
    "            target.append(timeframe//2+offset)\n",
    "    filter_array = np.array([False]*len_df)\n",
    "    filter_array[true_vals] = True\n",
    "    filter_array = pl.Series(filter_array)\n",
    "    return df.filter(filter_array), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c34a20c-5011-4845-beca-bd6b2879ee00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current split: fine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 137/137 [08:41<00:00,  3.81s/it]\n"
     ]
    }
   ],
   "source": [
    "splits = {'fine': to_keep+target_col}\n",
    "timeframe = 1440 #2 hours timeframe\n",
    "event = pl.read_csv('events_compressed.csv')\n",
    "\n",
    "for split,cols in splits.items():\n",
    "    print(f'Current split: {split}')\n",
    "    train = pl.scan_parquet(source = 'train_label.parquet',) #length of 127.946.340\n",
    "    chuncks = []\n",
    "    targets = []\n",
    "    for idt in tqdm(range(140,277)):\n",
    "        id_df = train.filter(pl.col('series_id') == idt).collect()\n",
    "        eng_df = custom_feat_eng(id_df,cols)\n",
    "        eng_df, target = get_timeframe(eng_df, timeframe, event, idt, len(id_df))\n",
    "        chuncks.append(eng_df)\n",
    "        targets += target\n",
    "    train = pl.concat(chuncks, how = 'vertical').collect()\n",
    "    train.write_parquet(f\"/sleep/train_{split}_{timeframe}_271.parquet\")\n",
    "    with open(\"target.txt\", \"a\") as file:\n",
    "        for item in targets:\n",
    "            file.write(str(item) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
