{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db8e6c76-3178-45ba-80ae-330437b53b19",
   "metadata": {},
   "source": [
    "<font size =\"10\"> Data Engeneering</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a9f41a-4331-48d8-a241-8e524fe3264b",
   "metadata": {},
   "source": [
    "Transform in Multilabel classification with \"awake\", \"asleep\", \"not wearing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "314cb156-aca7-4923-a2d4-5c102a5f0db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "pd.set_option('mode.chained_assignment', None) #Avoids warning\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.notebook import trange\n",
    "from pandarallel import pandarallel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import stumpy\n",
    "from stumpy.floss import _cac\n",
    "import gc\n",
    "import multiprocessing \n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import optuna.integration.lightgbm as olgb\n",
    "import lightgbm as lgb\n",
    "import sklearn\n",
    "from lightgbm import early_stopping\n",
    "from lightgbm import log_evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nancorrmp.nancorrmp import NaNCorrMp\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a653e7-6b36-46a0-b721-7404e8634158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check values for days \n",
    "#train = pd.read_parquet(\"train_compressed.parquet\")\n",
    "train = pd.read_parquet(\"train_label.parquet\")\n",
    "train.drop(columns = ['state'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4345d5-07d9-43f6-ad91-cf7ac06ab42e",
   "metadata": {},
   "source": [
    "<font size =\"7\"> First work with the events file to build the necessary labels</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60d7c6f5-6014-4291-b0d8-b5907a9a6d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the events data from multiple wakeup/onset on the same night\n",
    "events = pd.read_csv(\"events_compressed.csv\")\n",
    "fault_indeces = []\n",
    "for idx in range(0, len(events) - 1):\n",
    "    \n",
    "    logic1 = (events.loc[idx, 'event'] == events.loc[idx + 1, 'event'])\\\n",
    "            and (events.loc[idx, 'night'] == events.loc[idx + 1, 'night']) \n",
    "    \n",
    "    logic2 = (events.loc[idx, 'night'] != events.loc[idx + 1, 'night'])\\\n",
    "            and (events.loc[idx, 'night'] != events.loc[idx - 1, 'night'])\n",
    "    \n",
    "    if logic1 or logic2:\n",
    "        fault_indeces.append(idx)\n",
    "\n",
    "events.drop(fault_indeces, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f757346-d104-4236-859f-8789f2891cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add label 3 (\"not worn\") for the nights where no event was recorded\n",
    "new_user_col = []\n",
    "new_night_col = []\n",
    "for i in range(277):\n",
    "    user = events[events[\"series_id\"]==i]\n",
    "    nights = list(user[\"night\"].unique())\n",
    "    train_user = train[train[\"series_id\"]==i]\n",
    "    total_nights = (train_user[\"timestamp\"].max()-train_user[\"timestamp\"].min()).days\n",
    "    missing_nights = [x for x in list(range(1,total_nights+1)) if x not in nights]\n",
    "    new_night_col.extend(missing_nights)\n",
    "    new_user_col.extend([i]*len(missing_nights))\n",
    "    \n",
    "df = pd.DataFrame({\"series_id\": new_user_col, \"night\": new_night_col, \"event\": [3]*len(new_user_col)\n",
    ", \"step\": [None]*len(new_user_col), \"timestamp\": [None]*len(new_user_col)})\n",
    "\n",
    "events_worn = pd.concat([events, df]).sort_values(by=['series_id', 'night'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c591fa7a-70cf-48fc-a933-f8a942adcf2a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#For a given timestep and delta returns all the rows in that timeframe, then checks in a rolling window the \n",
    "#difference between the largest and smallest ENMO and if below a thresholds then takes the earliest/latest event\n",
    "#and assigns to it the labels taken off/worn \n",
    "def get_sliding_window(series_id, time1, delta, event):\n",
    "    idx = None\n",
    "    window_size = 720 #1.5 hours with no activity, hyperparameters that can be modified(together with the max/min) in the lines below\n",
    "    threshold = 0.1\n",
    "    if event == 2:\n",
    "        df = train[(train[\"series_id\"] == series_id) & (train[\"timestamp\"] > time1) & (train[\"timestamp\"] < time1+delta)] \n",
    "    else:\n",
    "        df = train[(train[\"series_id\"] == series_id) & (train[\"timestamp\"] < time1) & (train[\"timestamp\"] > time1-delta)]\n",
    "\n",
    "    df[\"window\"] = df[\"enmo\"].rolling(window_size).max()-df[\"enmo\"].rolling(window_size).min()\n",
    "    zero_window_indices = df[df[\"window\"] < threshold].timestamp\n",
    "\n",
    "    #Situation where no valid rest period was detected, force the search\n",
    "    while len(zero_window_indices) == 0:\n",
    "        threshold += 0.1\n",
    "        window = df[\"enmo\"].rolling(window_size)\n",
    "        df[\"window\"] = window.max()-window.min()\n",
    "        zero_window_indices = df[df[\"window\"] < threshold].timestamp\n",
    "        \n",
    "    if event == 2:\n",
    "        idx = zero_window_indices.min()-timedelta(seconds=(window_size-1)*5) #Because the index of rolling window is to the right\n",
    "    else:\n",
    "        idx = zero_window_indices.max()\n",
    "    step = df[df['timestamp'] == idx].iloc[0]['step']\n",
    "    return idx, step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9f7f4d4-1ef6-4b80-9b49-73f72869ffb0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd64ce62d7f449ab298118c37625275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/277 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Detect when the watch was taken off/worn again between the nights where an event was recorded and none and then add a label to these events\n",
    "timestamps = []\n",
    "events = []\n",
    "steps = []\n",
    "nights = []\n",
    "series_ids = []\n",
    "for i in trange(277):\n",
    "    user = events_worn[events_worn[\"series_id\"]==i]\n",
    "    filtered_timestamps = user[((user['event'].shift(-1) == 3) | (user['event'].shift(1) == 3)) & (user['event'] != 3)]\n",
    "    filtered_timestamps[\"timestamp\"] = pd.to_datetime(filtered_timestamps[\"timestamp\"], format='%Y-%m-%dT%H:%M:%S%z')\n",
    "    time_event = filtered_timestamps[[\"timestamp\",\"event\"]]\n",
    "    for j in range(len(time_event)):\n",
    "        timestamp,step = get_sliding_window(i,time_event.iloc[j][\"timestamp\"],timedelta(hours=24),time_event.iloc[j][\"event\"])\n",
    "        timestamps.append(timestamp)\n",
    "        night = filtered_timestamps.iloc[j][\"night\"]\n",
    "        if time_event.iloc[j][\"event\"] == 2:\n",
    "            events.append(4)\n",
    "            nights.append(night+1)\n",
    "        else:\n",
    "            events.append(5)\n",
    "            nights.append(night-1)\n",
    "        series_ids.append(i)\n",
    "        steps.append(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79203bed-8780-469e-939a-779627335a0c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       series_id  night  event   step           timestamp\n",
      "0              0      1      1   4992 2018-08-14 22:26:00\n",
      "1              0      1      2  10932 2018-08-15 06:41:00\n",
      "2              0      2      1  20244 2018-08-15 19:37:00\n",
      "3              0      2      2  27492 2018-08-16 05:41:00\n",
      "4              0      3      1  39996 2018-08-16 23:03:00\n",
      "5              0      3      2  44400 2018-08-17 05:10:00\n",
      "6              0      4      1  57240 2018-08-17 23:00:00\n",
      "7              0      4      2  62856 2018-08-18 06:48:00\n",
      "12084          0      5      3  74000 2018-08-18 22:16:40\n",
      "12085          0      5      4  81772 2018-08-19 09:04:20\n"
     ]
    }
   ],
   "source": [
    "#Add the collected data to the labels and drop the label 3 \n",
    "new_labels = {\"series_id\":series_ids, \"night\": nights, \"event\" : events, \"step\": steps, \"timestamp\": timestamps}\n",
    "new_df = pd.DataFrame(new_labels)\n",
    "concatenated_df = pd.concat([events_worn, new_df], ignore_index=True)\n",
    "concatenated_df['timestamp'] = pd.to_datetime(concatenated_df['timestamp'], format='%Y-%m-%dT%H:%M:%S%z')\n",
    "sorted_df = concatenated_df.sort_values(by=['series_id','timestamp'])\n",
    "sorted_df= sorted_df[sorted_df['event'] != 3]\n",
    "sorted_df['event'] = sorted_df['event'].replace({4:3, 5:4})\n",
    "print(sorted_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12a6dcfd-73ef-46d6-8f45-d5ef36109506",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.to_csv(\"events_modified.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d16a9a19-1500-4b35-8533-ab269d265e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c746b673a78c4fae94f4d537ac59f931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/277 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Provide the label to the training data 0 = awake, 1 = sleeping, 2 = not wearing, based on the events_modified\n",
    "events = pd.read_csv(\"events_modified.csv\")\n",
    "\n",
    "def process_user(chunk):\n",
    "    i = chunk.iloc[0][\"series_id\"]\n",
    "    user = events[events[\"series_id\"] == i] #Concurrent readin, shouldn't be a problem\n",
    "    if len(user) == 0:\n",
    "        chunk['state'] = 2  # No data recording-> Never worn the bracelet\n",
    "    else:\n",
    "        for j in range(len(user) - 1):\n",
    "            event = user.iloc[j][\"event\"]\n",
    "            timestamp = user.iloc[j][\"timestamp\"]\n",
    "            next_event = user.iloc[j + 1][\"event\"]\n",
    "            next_timestamp = user.iloc[j + 1][\"timestamp\"]\n",
    "\n",
    "            if j + 1 == len(user) - 1:  # corner case where we are in the last entry\n",
    "                if next_event == 2:  # Woke up in the last day\n",
    "                    chunk.loc[(chunk[\"timestamp\"] > next_timestamp), 'state'] = 1\n",
    "                else:  # Not worn the last days\n",
    "                    chunk.loc[(chunk[\"timestamp\"] > next_timestamp), 'state'] = 2\n",
    "            if j == 0:  # First case, need to check what is the first event\n",
    "                if event == 1:  # Went to sleep at first night, so he was awake before\n",
    "                    chunk.loc[(chunk[\"timestamp\"] < timestamp), 'state'] = 0\n",
    "                else:  # No event recorded in the first nights, so not wearing\n",
    "                    chunk.loc[(chunk[\"timestamp\"] < timestamp), 'state'] = 2\n",
    "            # Other cases\n",
    "            if (event == 1 and next_event == 2):  # Sleeping case\n",
    "                chunk.loc[(chunk[\"timestamp\"] >= timestamp) & (\n",
    "                            chunk[\"timestamp\"] <= next_timestamp), 'state'] = 1\n",
    "            elif (event == 2 and next_event == 1) or (event == 2 and next_event == 3) or (\n",
    "                    event == 4 and next_event == 1):  # Awake case\n",
    "                chunk.loc[(chunk[\"timestamp\"] >= timestamp) & (\n",
    "                            chunk[\"timestamp\"] <= next_timestamp), 'state'] = 0\n",
    "            elif (event == 3 and next_event == 4):  # Not Wearing case\n",
    "                chunk.loc[(chunk[\"timestamp\"] >= timestamp) & (\n",
    "                            chunk[\"timestamp\"] <= next_timestamp), 'state'] = 2\n",
    "            else:  # Check in case of mistakes\n",
    "                print(user)\n",
    "                print(timestamp)\n",
    "    chunck[\"state\"] = chunck[\"state\"].astype(np.unit8)\n",
    "    return chunk\n",
    "\n",
    "# Create a list of series_id values to process\n",
    "chunks = [train[train['series_id'] == i] for i in range(277)]\n",
    "\n",
    "# Set up multiprocessing pool\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "pool = multiprocessing.Pool(processes=num_processes)\n",
    "results = []\n",
    "\n",
    "# Process users in parallel\n",
    "with tqdm(total=len(chunks)) as pbar:\n",
    "    for result in pool.imap_unordered(process_user, chunks):\n",
    "        results.append(result) \n",
    "        pbar.update(1)\n",
    "\n",
    "# Close the pool\n",
    "pool.close()\n",
    "pool.join()\n",
    "train = pd.concat(results).sort_values(by=[\"series_id\",\"step\"]) #Avoid fragmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f75c1-8dd2-4105-b21a-7da8e7b311f3",
   "metadata": {},
   "source": [
    "<font size =\"7\"> Now adding features to the train/test data (With Pandas, use polars for faster results)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1d5ed7-5b5a-424c-a6a1-843b2e0169fc",
   "metadata": {},
   "source": [
    "Features List (Rows):\n",
    "signal_awake = Wave that represents the awake period probability (flaot 32)\n",
    "signal_sleep = Wave that represents the sleep period probability (flaot 32)\n",
    "lids = Not sure what it does (float 32)\n",
    "[mean,median,max,min] apply these rolling windows (make some different sizes) operations plus others on the columns ['enmo', 'anglez', 'anglez_x_enmo', 'anglezabs', 'anglez_diff', \"anglez_diffabs\"]\n",
    "anglez_diff = Difference between previous entry and new entry (float 32)\n",
    "anglez_diffabs = Absolute difference between previous entry and new entry (float 32)\n",
    "anglez_x_enmo = Anglez * Enmo (float 32)\n",
    "[std] on 'anglez' and 'anglez_diffabs'\n",
    "[month,day,hour,minute]\n",
    "mp_enmo = Matrix Profile\n",
    "mp_angle = Matrix Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7fd4248-a26e-448f-b15e-f78f0f84d7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_datetime64_ns_dtype(dataframe, column_name):\n",
    "    # Get the dtype of the specified column\n",
    "    dtype = dataframe[column_name].dtype\n",
    "    \n",
    "    # Check if the dtype is not datetime64\n",
    "    return pd.api.types.is_datetime64_ns_dtype(dtype)\n",
    "    \n",
    "def reduce_mem_usage(df):\n",
    "    \n",
    "    \"\"\" \n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object and not is_datetime64_ns_dtype(df, col) and not 'category':\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.uint8).min and c_max < np.iinfo(np.uint8).max:\n",
    "                    df[col] = df[col].astype(np.uint8)\n",
    "                elif c_min > np.iinfo(np.uint16).min and c_max < np.iinfo(np.uint16).max:\n",
    "                    df[col] = df[col].astype(np.uint16)\n",
    "                elif c_min > np.iinfo(np.uint32).min and c_max < np.iinfo(np.uint32).max:\n",
    "                    df[col] = df[col].astype(np.uint32)\n",
    "                elif c_min > np.iinfo(np.uint64).min and c_max < np.iinfo(np.uint64).max:\n",
    "                    df[col] = df[col].astype(np.int32)  \n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float16)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e33b458-9a69-4c26-bcef-503b99985aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_eng(df):\n",
    "    \n",
    "    df.sort_values(['timestamp'], inplace=True)\n",
    "    \n",
    "    #BASE SIGNALS\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    df['lids'] = np.maximum(0., df['enmo'] - 0.02)\n",
    "    df['lids'] = df['lids'].rolling(f'{120*5}s', center=True, min_periods=1).agg('sum')\n",
    "    df['lids'] = 100 / (df['lids'] + 1)\n",
    "    df['lids'] = df['lids'].rolling(f'{360*5}s', center=True, min_periods=1).agg('mean').astype(np.float32)\n",
    "    \n",
    "    df[\"enmo_diff\"] = df[\"enmo\"].diff().astype(np.float32)\n",
    "    df[\"enmo_diffabs\"] = abs(df[\"enmo_diff\"]).astype(np.float32)\n",
    "    df['anglez_x_lids'] = (df['anglez'] * df['lids']).astype(np.float32)\n",
    "    df[\"lids_diff\"] = df[\"lids\"].diff().astype(np.float32)\n",
    "    df[\"lids_diffabs\"] = abs(df[\"lids_diff\"]).astype(np.float32)\n",
    "    df['enmo_x_lids'] = (df['enmo'] * df['lids']).astype(np.float32)\n",
    "    \n",
    "    \n",
    "    for col in [\"enmo_diff\", \"enmo_diffabs\",'anglez_x_lids',\"lids_diff\",\"lids_diffabs\",'enmo_x_lids']:\n",
    "        \n",
    "        for n in [51]:\n",
    "            df[f'{col}_diff_{n}'] = df[col].diff(periods=n).astype(np.float32)\n",
    "        \n",
    "            rol_args = {'window':f'{n*5}s', 'min_periods':1, 'center':True}\n",
    "            \n",
    "            for agg in ['median', 'mean', 'max', 'min']:\n",
    "                df[f'{col}_{agg}_{n}'] = df[col].rolling(**rol_args).agg(agg).astype(np.float32).values\n",
    "                gc.collect()\n",
    "                \n",
    "            df[f'{col}_mad_{n}'] = (df[col] - df[f'{col}_median_{n}']).abs().rolling(**rol_args).median().astype(np.float32)\n",
    "\n",
    "            df[f'{col}_amplit_{n}'] = df[f'{col}_max_{n}']-df[f'{col}_min_{n}']\n",
    "            df[f'{col}_diff_{n}_max'] = df[f'{col}_max_{n}'].rolling(**rol_args).max().astype(np.float32)\n",
    "            \n",
    "            if col in ['anglez', 'anglez_diffabs']:\n",
    "                df[f'{col}_{agg}_{n}'] = df[col].rolling(**rol_args).std().astype(np.float32)\n",
    "    \n",
    "            gc.collect()\n",
    "        \n",
    "#         df[f'conv1d_{col}']\n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "    df.bfill(inplace=True)\n",
    "    df.ffill(inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4431ef6f-f006-4ea2-9c96-cfc79a7bba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_awake = dict(zip(range(1440), np.sin(np.linspace(0, np.pi, 1440) + 0.208 * np.pi) ** 24))\n",
    "signal_onset = dict(zip(range(1440), np.sin(np.linspace(0, np.pi, 1440) + 0.555 * np.pi) ** 24))\n",
    "#Might modify the 2 hyperparameters\n",
    "\n",
    "def custom_feat_eng(df):\n",
    "    \n",
    "    #Time related features\n",
    "    #df['series_id'] = df['series_id'].astype('category')\n",
    "    #df['timestamp'] = pd.to_datetime(df['timestamp']).apply(lambda t: t.tz_localize(None))\n",
    "    df['month'] = df['timestamp'].dt.month.astype(np.uint8)\n",
    "    df['day'] = df['timestamp'].dt.day.astype(np.uint8)\n",
    "    df['hour'] = df['timestamp'].dt.hour.astype(np.uint8)\n",
    "    df['minute'] = df['timestamp'].dt.minute.astype(np.uint8)\n",
    "    df['day_of_week'] = df['timestamp'].dt.dayofweek.astype(np.uint8)\n",
    "    df.sort_values(['timestamp'], inplace=True)\n",
    "    \n",
    "    df['signal_onset'] = (df.timestamp.dt.hour * 60 + df.timestamp.dt.minute).map(signal_onset).astype(np.float32)\n",
    "    df['signal_awake'] = (df.timestamp.dt.hour * 60 + df.timestamp.dt.minute).map(signal_awake).astype(np.float32)\n",
    "\n",
    "    #BASE SIGNALS\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    df['lids'] = np.maximum(0., df['enmo'] - 0.02)\n",
    "    df['lids'] = df['lids'].rolling(f'{120*5}s', center=True, min_periods=1).agg('sum')\n",
    "    df['lids'] = 100 / (df['lids'] + 1)\n",
    "    df['lids'] = df['lids'].rolling(f'{360*5}s', center=True, min_periods=1).agg('mean').astype(np.float32)\n",
    "    \n",
    "    #Additional Features\n",
    "    for col in ['enmo', 'anglez', 'lids']:\n",
    "        #Base new features\n",
    "        n = 12\n",
    "        rol_args = {'window':f'{n*5}s', 'min_periods':1, 'center':True}\n",
    "        for agg in ['median', 'mean', 'max', 'min', 'std']:   \n",
    "            df[f'{col}_{agg}_{n*5}'] = df[col].rolling(**rol_args).agg(agg).astype(np.float32)\n",
    "        df[f'{col}_mad_{n*5}'] = (df[col] - df[f'{col}_median_{n*5}']).abs().rolling(**rol_args).median().astype(np.float32) #Median absolute deviation\n",
    "        df[f'{col}_amplit_{n*5}'] = df[f'{col}_max_{n*5}']-df[f'{col}_min_{n*5}']    \n",
    "        df.drop(columns = [f'{col}_median_{n*5}'], inplace = True)\n",
    "        \n",
    "        #Mean differences across different intervals\n",
    "        for i in [12,24,120]:\n",
    "            rol_args = {'window':f'{i*5}s', 'min_periods':1, 'center':True}\n",
    "            df[f'{col}_next_{i*5}_diff'] = df[col].rolling(**rol_args).mean().diff(periods=n+1)\n",
    "            df[f'{col}_prev_{i*5}_diff'] = df[col].rolling(**rol_args).mean().diff(periods=-n+1)\n",
    "            gc.collect()\n",
    "\n",
    "        #Entropy measure\n",
    "        #Entropy me\n",
    "        for b in [20,200]:\n",
    "            hist, bin_edges = np.histogram(df[col], bins=b)\n",
    "            probabilities = hist / np.sum(hist)\n",
    "            df[f'entropy_{b}_{col}'] = np.digitize(df[col], bin_edges[:-1])-1\n",
    "            translate = {i:value for i,value in enumerate(probabilities)}\n",
    "            df[f'entropy_{b}_{col}'] = df[f'entropy_{b}_{col}'].map(translate)\n",
    "            df[f'entropy_{b}_{col}'] = -df[f'entropy_{b}_{col}']*np.log(df[f'entropy_{b}_{col}'])\n",
    "            gc.collect() \n",
    "\n",
    "    #Seconds since/until previous/next significant movement and how many in rolling window as defined by percentile (on enmo)  \n",
    "    for p in [0.6, 0.9, 0.99]:\n",
    "        quantile = df['enmo'].quantile(p)\n",
    "        steps_high = df[df['enmo']>quantile].step \n",
    "        steps_high = pd.concat([pd.Series(0),steps_high,pd.Series([len(df)])])\n",
    "        to_next = steps_high.diff(1).tolist()  \n",
    "        steps_to_next = []\n",
    "        steps_to_prev = []\n",
    "        for i,val in enumerate(to_next[1:]):\n",
    "            val = int(val)\n",
    "            to_extend = range(0,val)\n",
    "            if i == 0:     \n",
    "                half_val = int(val/2)\n",
    "                steps_to_next.extend(reversed(to_extend))\n",
    "                steps_to_prev.extend(range(0+half_val,val+half_val))\n",
    "                continue\n",
    "            if i == len(to_next)-2: \n",
    "                half_val = int(val/2)\n",
    "                steps_to_next.extend(reversed(range(0+half_val,val+half_val)))\n",
    "                steps_to_prev.extend(to_extend)\n",
    "                break\n",
    "            steps_to_next.extend(reversed(to_extend))\n",
    "            steps_to_prev.extend(to_extend)\n",
    "\n",
    "        #Code encounters some errors with None values, therefore try to correct\n",
    "        last_next = 100\n",
    "        last_prev = 100\n",
    "        for item in reversed(steps_to_next):\n",
    "            if item is not None:\n",
    "                last_next = item\n",
    "                break\n",
    "        for item in reversed(steps_to_prev):\n",
    "            if item is not None:\n",
    "                last_prev = item    \n",
    "                break                 \n",
    "        df[f'steps_to_next_{p}'] = steps_to_next.extend(reversed(range(last_next,last_next+(len(df)-len(steps_to_next)))))\n",
    "        df[f'steps_to_prev_{p}'] = steps_to_prev.extend(range(last_prev,last_prev+(len(df)-len(steps_to_prev))))\n",
    "        df[f'steps_to_next_{p}'] = df[f'steps_to_next_{p}'].fillna(last_next).astype(np.uint32)\n",
    "        df[f'steps_to_prev_{p}'] = df[f'steps_to_prev_{p}'].fillna(last_prev).astype(np.uint32)\n",
    "\n",
    "        \n",
    "        df[f'log_steps_to_next_{p}'] = np.log1p(df[f'steps_to_next_{p}']).astype(np.float32)\n",
    "        df[f'log_steps_to_prev_{p}'] = np.log1p(df[f'steps_to_prev_{p}']).astype(np.float32)\n",
    "        \n",
    "        for i in [24,120,400]:   \n",
    "            rol_args = {'window':f'{i*5}s', 'min_periods':1, 'center':True}\n",
    "            df[f'amount_of_movement_{p}_in_last{i*5}'] = df['enmo']>quantile\n",
    "            df[f'amount_of_movement_{p}_in_last{i*5}'] = df[f'amount_of_movement_{p}_in_last{i*5}'].rolling(**rol_args).sum().astype(np.float32)\n",
    "            df[f'amount_of_movement_{p}_in_last{i*5}'] = np.log1p(df[f'amount_of_movement_{p}_in_last{i*5}']).astype(np.float32)\n",
    "    \n",
    "    \"\"\"\n",
    "    ############################ Matrix Profile(Computationally too expensive) ##############################################\n",
    "    window_size = 5760\n",
    "    nan_values = np.full(window_size-1, np.nan)\n",
    "    stump = stumpy.scrump(df['enmo'].astype(np.float64).values, m=window_size, pre_scrump = True) #Window size of 8 hours\n",
    "    df['enmo_mp'] = np.append(stump.P_, nan_values)\n",
    "    df['enmo_mp'] = df['enmo_mp'].astype(np.float16)\n",
    "    df['enmo_i'] = np.append(stump.I_, nan_values)\n",
    "    df['enmo_i'] = df['enmo_i'].astype(np.float16)\n",
    "    stump = stumpy.scrump(df['anglez'].astype(np.float64).values, m=window_size, pre_scrump = True)\n",
    "    df['anglez_mp'] = np.append(stump.P_, nan_values)\n",
    "    df['anglez_mp'] = df['anglez_mp'].astype(np.float16)\n",
    "    df['anglez_i'] = np.append(stump.I_, nan_values)\n",
    "    df['anglez_i'] = df['anglez_i'].astype(np.float16)\n",
    "    \"\"\"\n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "    df.ffill(inplace=True)\n",
    "    df.bfill(inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbca34b-cfca-43c7-ab81-69eba3172996",
   "metadata": {},
   "source": [
    "Other Feature Engeneering approaches:\n",
    "https://www.kaggle.com/code/zotovaa/cmi-gradient-boosting-new-parameters\n",
    "https://www.kaggle.com/code/lccburk/feature-engineering-and-random-forest-prediction\n",
    "https://www.kaggle.com/code/zhukovoleksiy/detect-sleep-states-starter-notebook-ensemble\n",
    "https://www.kaggle.com/code/renatoreggiani/feature-eng-ideas-and-lightgbm-cmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a483da61-ea12-451b-b81f-11769267449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"train_label.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a05b03c-bb6c-430e-a4d8-a39cfea7df09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the results chunks from the previous session\n",
    "chunks = [train[train['series_id'] == i] for i in range(277)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6262a7-9548-41ce-b48a-a24276709a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14780bed5017425d8a8c687249c3b698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/277 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_processes = multiprocessing.cpu_count()\n",
    "pool = multiprocessing.Pool(processes=num_processes)\n",
    "results = []\n",
    "with tqdm(total=len(chunks)) as pbar:\n",
    "    for result in pool.imap_unordered(feat_eng, chunks):\n",
    "        results.append(result) \n",
    "        pbar.update(1)\n",
    "\n",
    "# Close the pool\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8245505c-c173-4633-8793-8f11a74f0bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del chunks\n",
    "del train\n",
    "gc.collect()\n",
    "train_mod = pd.concat(results).sort_values(by=[\"series_id\",\"step\"])\n",
    "del results\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc04f7e5-9d5c-4710-b33d-0287c4be19fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mod.to_parquet(\"train3.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f82117d9-8b94-467a-80c1-64640d945205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp                  datetime64[ns]\n",
       "series_id                          uint16\n",
       "step                               uint32\n",
       "anglez                            float32\n",
       "enmo                              float32\n",
       "                                ...      \n",
       "enmo_x_lids_diff_51_max           float32\n",
       "entropy_20_enmo                   float64\n",
       "entropy_200_enmo                  float64\n",
       "entropy_20_anglez                 float64\n",
       "entropy_200_anglez                float64\n",
       "Length: 65, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mod.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "409116c5-327d-443c-99f4-65f9213fff57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['timestamp', 'series_id', 'step', 'anglez', 'enmo', 'state', 'lids',\n",
      "       'enmo_diff', 'enmo_diffabs', 'anglez_x_lids', 'lids_diff',\n",
      "       'lids_diffabs', 'enmo_x_lids', 'enmo_diff_diff_51',\n",
      "       'enmo_diff_median_51', 'enmo_diff_mean_51', 'enmo_diff_max_51',\n",
      "       'enmo_diff_min_51', 'enmo_diff_mad_51', 'enmo_diff_amplit_51',\n",
      "       'enmo_diff_diff_51_max', 'enmo_diffabs_diff_51',\n",
      "       'enmo_diffabs_median_51', 'enmo_diffabs_mean_51', 'enmo_diffabs_max_51',\n",
      "       'enmo_diffabs_min_51', 'enmo_diffabs_mad_51', 'enmo_diffabs_amplit_51',\n",
      "       'enmo_diffabs_diff_51_max', 'anglez_x_lids_diff_51',\n",
      "       'anglez_x_lids_median_51', 'anglez_x_lids_mean_51',\n",
      "       'anglez_x_lids_max_51', 'anglez_x_lids_min_51', 'anglez_x_lids_mad_51',\n",
      "       'anglez_x_lids_amplit_51', 'anglez_x_lids_diff_51_max',\n",
      "       'lids_diff_diff_51', 'lids_diff_median_51', 'lids_diff_mean_51',\n",
      "       'lids_diff_max_51', 'lids_diff_min_51', 'lids_diff_mad_51',\n",
      "       'lids_diff_amplit_51', 'lids_diff_diff_51_max', 'lids_diffabs_diff_51',\n",
      "       'lids_diffabs_median_51', 'lids_diffabs_mean_51', 'lids_diffabs_max_51',\n",
      "       'lids_diffabs_min_51', 'lids_diffabs_mad_51', 'lids_diffabs_amplit_51',\n",
      "       'lids_diffabs_diff_51_max', 'enmo_x_lids_diff_51',\n",
      "       'enmo_x_lids_median_51', 'enmo_x_lids_mean_51', 'enmo_x_lids_max_51',\n",
      "       'enmo_x_lids_min_51', 'enmo_x_lids_mad_51', 'enmo_x_lids_amplit_51',\n",
      "       'enmo_x_lids_diff_51_max', 'entropy_20_enmo', 'entropy_200_enmo',\n",
      "       'entropy_20_anglez', 'entropy_200_anglez'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_mod.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
